{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from NeuralNetworks.layers import FullyConnected\n",
    "from NeuralNetworks.activations import Linear, Sigmoid, TanH, ReLU, SoftMax\n",
    "from grad_check import check_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for Linear activation: 2.8434717441750492e-11\n",
      "Relative error for Sigmoid activation: 1.1071217121012581e-10\n",
      "Relative error for TanH activation: 2.2266722710042676e-11\n",
      "Relative error for ReLU activation: 1.362101106828521e-11\n",
      "Relative error for Softmax activation: 1.1238822943861714e-10\n"
     ]
    }
   ],
   "source": [
    "check = {'Linear': Linear, 'Sigmoid': Sigmoid, 'TanH': TanH, 'ReLU': ReLU, 'Softmax': SoftMax}\n",
    "\n",
    "for name, fn in check.items(): \n",
    "    X = np.random.randn(2, 3)\n",
    "    dLdY = np.random.randn(2, 3)\n",
    "\n",
    "    # initialize a fully connected layer\n",
    "    # and perform a forward and backward pass\n",
    "    activation = fn()\n",
    "    _ = activation.forward(X)\n",
    "    grad = activation.backward(X, dLdY)\n",
    "\n",
    "    # check the gradients w.r.t. each parameter\n",
    "    print(\n",
    "        f\"Relative error for {name} activation:\",\n",
    "        check_gradients(\n",
    "            fn=activation.forward,  # the function we are checking\n",
    "            grad=grad,  # the analytically computed gradient\n",
    "            x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,  # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for W: 1.4501579951504764e-11\n",
      "Relative error for b: 5.740241368282514e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "fc_layer = FullyConnected(n_out=4, activation=\"linear\")\n",
    "_ = fc_layer.forward(X)\n",
    "_ = fc_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in fc_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=fc_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=fc_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=fc_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
