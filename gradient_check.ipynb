{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical Gradient Checking \n",
    "please refer to `grad_check.py` to see the implementation of how numerical gradient checking is carried out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from NeuralNetworks.layers import FullyConnected\n",
    "from NeuralNetworks.activations import Linear, Sigmoid, TanH, ReLU, SoftMax\n",
    "from NeuralNetworks.losses import CrossEntropy\n",
    "from utils import check_gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Function Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for Linear activation: 4.407971522515095e-12\n",
      "Relative error for Sigmoid activation: 6.41001440675322e-11\n",
      "Relative error for TanH activation: 2.1801962031463618e-11\n",
      "Relative error for ReLU activation: 1.4377884969231344e-11\n",
      "Relative error for Softmax activation: 3.948456953048531e-11\n"
     ]
    }
   ],
   "source": [
    "check = {'Linear': Linear, 'Sigmoid': Sigmoid, 'TanH': TanH, 'ReLU': ReLU, 'Softmax': SoftMax}\n",
    "\n",
    "for name, fn in check.items(): \n",
    "    X = np.random.randn(2, 3)\n",
    "    dLdY = np.random.randn(2, 3)\n",
    "\n",
    "    # initialize a fully connected layer\n",
    "    # and perform a forward and backward pass\n",
    "    activation = fn()\n",
    "    _ = activation.forward(X)\n",
    "    grad = activation.backward(X, dLdY)\n",
    "\n",
    "    # check the gradients w.r.t. each parameter\n",
    "    print(\n",
    "        f\"Relative error for {name} activation:\",\n",
    "        check_gradients(\n",
    "            fn=activation.forward,  # the function we are checking\n",
    "            grad=grad,  # the analytically computed gradient\n",
    "            x=X,        # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,  # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Connected Layer Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for W: 4.0907738088780064e-11\n",
      "Relative error for b: 3.3474831817927246e-11\n"
     ]
    }
   ],
   "source": [
    "X = np.random.randn(2, 3)\n",
    "dLdY = np.random.randn(2, 4)\n",
    "\n",
    "# initialize a fully connected layer\n",
    "# and perform a forward and backward pass\n",
    "fc_layer = FullyConnected(n_out=4, activation=\"linear\")\n",
    "_ = fc_layer.forward(X)\n",
    "_ = fc_layer.backward(dLdY)\n",
    "\n",
    "# check the gradients w.r.t. each parameter\n",
    "for param in fc_layer.parameters:\n",
    "    print(\n",
    "        f\"Relative error for {param}:\",\n",
    "        check_gradients(\n",
    "            fn=fc_layer.forward_with_param(param, X),  # the function we are checking\n",
    "            grad=fc_layer.gradients[param],  # the analytically computed gradient\n",
    "            x=fc_layer.parameters[param],  # the variable w.r.t. which we are taking the gradient\n",
    "            dLdf=dLdY,                     # gradient at previous layer\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss Gradient Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relative error for cross entropy loss: 3.058921437577199e-09\n"
     ]
    }
   ],
   "source": [
    "num_pts = 5\n",
    "num_classes = 6\n",
    "\n",
    "# one-hot encoded y\n",
    "y_idxs = np.random.randint(0, num_classes, (num_pts,))\n",
    "y = np.zeros((num_pts, num_classes))\n",
    "y[range(num_pts), y_idxs] = 1\n",
    "\n",
    "# normalized predictions\n",
    "scores = np.random.uniform(0, 1, size=(num_pts, num_classes))\n",
    "y_hat = scores / scores.sum(axis=1, keepdims=True)\n",
    "\n",
    "cross_entropy_loss = CrossEntropy(\"cross_entropy\")\n",
    "\n",
    "def forward_fn(Y, Y_hat):    \n",
    "    def inner_forward(Y_hat):\n",
    "        return cross_entropy_loss.forward(Y, Y_hat)\n",
    "    return inner_forward\n",
    "\n",
    "loss = cross_entropy_loss.forward(y, y_hat)\n",
    "grad = cross_entropy_loss.backward(y, y_hat)\n",
    "\n",
    "print(\n",
    "    f\"Relative error for cross entropy loss:\",\n",
    "    check_gradients(\n",
    "        fn=forward_fn(y, y_hat),  # the function we are checking\n",
    "        grad=grad,  # the analytically computed gradient\n",
    "        x=y_hat,        # the variable w.r.t. which we are taking the gradient\n",
    "        dLdf=1,  # gradient at previous layer\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
